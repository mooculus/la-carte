\documentclass{ximera}

\input{../../preamble.tex}


\author{Bart Snapp}



\title{Vector spaces}


\begin{document}
\begin{abstract}
  We describe properties of vectors and matrices and discuss their
  implications.
\end{abstract}
\maketitle

\begin{quote}
  We shall not cease from exploration\\
  And the end of all our exploring\\
  Will be to arrive where we started\\
  And know the place for the first time.


  \hfill ---T.S.\ Eliot
\end{quote}


Data and their associated systems of equations are complicated. It is
not enough to simply solve these equations, we must understand the
solutions in a more abstract way. In this chapter we will introduce
various sets (or in this case, vector spaces) of vectors in the
context of solving equations. Then we will revist these ideas from
`higher ground,' and introduce the notion of a vector space.


%% TK: my take on this
A fundamental task in linear algebra is to solve a system of linear
equations associated with certain data:
\begin{equation*}
  \left\{ \,
    \begin{array}{*{9}{@{}c@{}}}
      a_{11} x_1 & {}+{} & a_{12} x_2 & {}+{} & \cdots & {}+{} & a_{1n} x_n & {}\mathrel{=}{} & b_1 \\
      a_{21} x_1 & {}+{} & a_{22} x_2 & {}+{} & \cdots & {}+{} & a_{2n} x_n & {}\mathrel{=}{} & b_2 \\
      \vdots         &       & \vdots         &      &   &       & \vdots         &                 & \vdots \\
      a_{n1} x_1 & {}+{} & a_{n2} x_2 & {}+{} & \cdots & {}+{} & a_{nn} x_n & {}\mathrel{=}{} & b_n
    \end{array}
    \, \right.
\end{equation*}
It is not enough to simply solve these equations; we must understand
the solutions in a more abstract way. Let us first write the system
above compactly as a matrix equation $A \vec{x} = \vec{b}$, where $A$
is the $n \times n$ matrix of coefficients, $\vec{x}$ is the
$n$-vector of the unknowns, and $\vec{b}$ is the $n$-vector of the
right-hand sides:
\[
  A =
  \begin{bmatrix}
    a_{11} & a_{12} & \cdots & a_{1n} \\
    a_{21} & a_{22} & \cdots & a_{2n} \\
    \vdots & \vdots & \ddots & \vdots \\
    a_{n1} & a_{n2} & \cdots & a_{nn}
  \end{bmatrix}, \quad
  \vec{x} =
  \begin{bmatrix}
    x_1 \\ x_2 \\ \vdots \\ x_n
  \end{bmatrix}, \quad
  \vec{b} =
  \begin{bmatrix}
    b_1 \\ b_2 \\ \vdots \\ b_n
  \end{bmatrix}.
\]

We then consider two scenarios depending on the zero-ness of the
right-hand sides:
\begin{description}
\item[Case: $\vec{b} = \vec{0}$.] In the special case where
  $\vec{b} = \vec{0}$, the system is said to be
  \dfn{homogeneous}. Since a homogeneous system is fully characterized
  by its coefficients, the solution set is called the \textit{null
    space of $A$}. Since $\vec{x} = \vec{0}$ satisfies
  $A\vec{x} = \vec{0}$ no matter what $A$ is, the null space of $A$
  always contains the zero vector and so is never empty.
\item[Case: $\vec{b} \neq \vec{0}$.] On the other hand, if
  $\vec{b} \neq \vec{0}$, the system $A\vec{x} = \vec{b}$ may not have a
  solution at all. The existence of solution depends on whether
  $\vec{b}$ lies in the \textit{column space of $A$}.
\end{description}

The two jargons introduced above, the null space and the column space
of $A$, both contain the word \textit{space}. This comes from the fact
that they are special and very fundamental examples of the
mathematical structure called the vector (sub)space. We will introduce
the notion of vector space in the context of solving system of linear
equations. Then we will revisit these ideas from `higher ground', and
introduce the notion of an abstract vector space.

\section{Homogeneous equations and the null space}
Suppose we have a system of equations:
\begin{align*}
  a_1 x + b_1 y + c_1 z &= d_1\\
  a_2 x + b_2 y + c_2 z &= d_2\\
  a_3 x + b_3 y + c_3 z &= d_3
\end{align*}
Where each $a_i$, $b_i$, $c_i$, and $d_i$ are real numbers. We can
learn about the solutions to this system of equations if we
\textit{change the problem} to the following:
\begin{align*}
  a_1 x + b_1 y + c_1 z &= 0  \\
  a_2 x + b_2 y + c_2 z &= 0  \\
  a_3 x + b_3 y + c_3 z &= 0
\end{align*}
Equations of this form are called \dfn{homogeneous equations}. We call
them \textit{homogeneous} because all the (nonzero) terms are
multiplied by the same \textbf{powers of variables}.
%% TK: I am not sure if I follow this explanation about why they are
%% called homogeneous, because I am not sure whether we can consider
%% the nonlinear system
%%
%%   a_i x^2 + b_i y^2 + c_i z^2 = k, i = 1, 2, 3
%%
%% as homogeneous. The homogeneity in the case of linear systems
%% are due to 1) linearity and 2) zero constant terms. In
%% particular, because of this, it follows that if x is a solution
%% of Ax = 0, then any scalar multiple of x is also a solution. As a
%% consequence, ker(A) is a subspace.

%% Maybe, it was meant to be written as ``because all the
%% (nonconstant) terms are multiplied by ...''?

\begin{question}
  Which of the following equations are homogeneous?
  \begin{selectAll}
    \choice{$2x^3 + y^2 - 3z = 0$}
    \choice[correct]{$2x^3 + xy^2 - 3y^2z = 0$} % nonlinear, homogeneous
    \choice{$2x + y - 3 = 0$}
    \choice[correct]{$2x + y - 3z = 0$} % linear, homogeneous
  \end{selectAll}
\end{question}



\begin{definition}
  The \dfn{null space} (or \dfn{kernel}) of an $m \times n$ matrix $M$ is
  the set of all solutions to $M\vec{x} = \vec{0}$. We can express
  this as
  \[
  \Null(M) = \{ \vec{x}\in\R^n: M\vec{x} = \vec{0}\}.
  \]
\end{definition}

\begin{remark}
  A subtle thing to note here is that it is not necessary for $M$ to be
  a square matrix as in the introduction. Another to note is that
  $\Null(M)$ is a collection of vectors with length $n$, which coincides
  with the number of columns of $M$.
\end{remark}


\begin{example}
  Consider the following homogenous system of equations.

  Explain why the null space has 0

  The sum

  scalar multiples

\end{example}





\section{Column space}

Given a matrix (not necessarily square)
\[
\begin{pmatrix}
  a_1 &  b_1  &  c_1 \\
  a_2 & b_2 & c_2 \\
  a_3 & b_3 & c_3
\end{pmatrix}
\]

\[
\begin{pmatrix} d_1 \\ d_2 \\ d_3 \end{pmatrix}=\begin{pmatrix} a_1 \\ a_2 \\ a_3 \end{pmatrix} x +
\begin{pmatrix} b_1 \\ b_2 \\ b_3 \end{pmatrix} y +
\begin{pmatrix} c_1 \\ c_2 \\ c_3 \end{pmatrix} z
\]
It's called the \textit{column space} because it's equal to every
possible vector produced by the columns of the augmented matrix
associated to the system of equations:
Given a system of equations
\begin{align*}
  a_1 x + b_1 y + c_1 z &= d_1\\
  a_2 x + b_2 y + c_2 z &= d_2\\
  a_3 x + b_3 y + c_3 z &= d_3
\end{align*}
the \dfn{column space} is the set of all possible constants, expressed
as a single vector:





\section{Linear combinatrions of vectors}

\begin{definition}\index{K-vector space@$K$-vector space}\index{vector space@$K$-vector space}
  A \textbf{$\boldsymbol{K}$-vector space} is an Abelian group $(V,+)$
  with identity $\vec{0}$, along with a field $K$ such that we may
  multiply group elements by field elements, meaning that there is a
  binary operation $-\cdot-: K\times V \to V$ such that if $\nu,\mu\in
  V$ and $a,b,\in K$ we have:
\begin{description}
\item[Compatibility with scalars] $(ab)\cdot \nu = a\cdot (b\cdot \nu)$.
\item[Vectors distribute over scalars] $(a+b)\cdot \nu =
  a\cdot\nu + b\cdot \nu$.
\item[Scalars distribute over vectors] $a\cdot (\nu+\mu) =
  a\cdot \nu + a\cdot \mu$.
\item[Identity is respected] $1_K\cdot \nu = \nu$.
\end{description}
In this case, elements of the group $V$ are called \dfn{vectors} and
elements of the field $K$ are called \dfn{scalars}.
\end{definition}

\begin{lemma}[Subspace criterion]\index{subspace criterion}
  Let $V$ be a $K$-vector space. $W\subset V$ is a subspace of $V$ if
  and only if
  \begin{enumerate}
  \item $W\ne \emptyset$.
  \item $W$ is closed under multiplication by scalars.
  \item $W$ is closed under vector addition.
  \end{enumerate}
\end{lemma}

\subsection{Span of vectors}

\begin{definition}
  Given a set of vectors $S$, in a $K$-vector space, $V$, the
  \dfn{span} of the vectors in $S$ is
  \[
  \Span(S) = \left\{\sum_{i=1}^n a_i\sigma_i:\text{$n\in \N$,
    $\sigma_i\in S$, and $a_i\in K$}\right\}.
  \]
  If $\Span(S) = V$, then we say $S$ is a \dfn{spanning set}.
\end{definition}

\subsection{Linear indendence of vectors}


\begin{definition}
  Given a $K$-vector space $V$, a finite set of vectors
  \[
  \{\lambda_1,\dots,\lambda_n\}
  \]
  is said to be \dfn{linearly independent} if
  \[
  a_1\lambda_1 + a_2\lambda_2 +\cdots + a_n\lambda_n = 0\quad \Rightarrow \quad a_1= \cdots =a_n = 0.
  \]
  A finite set of vectors is set to be \dfn{linearly dependent} if
  they are not linearly independent.
\end{definition}



\section{Row space}

\section{Column space and range of a matrix}

\subsection{Singular matrices}

\end{document}
